记录一些在编辑电子书时候的感想，感觉通过这些点可以搞出来一个很好用的影印版转电子书的工具

感觉上最佳的处理流程应该是
1. 通过灰度，二值化等手段提纯信息
2. 通过比较智能的手段，识别内容块，比如标题，正文，图片等
3. 单独对内容块做提取，写入 markdown 并加入特征信息，比如标题字号等

## 23-03-11

本次实现的简单脚本，已经可以识别绝大部分的中文字了。不过还是有一些缺陷感觉可以改进。
* 标题是直接以普通文字识别的，并没有特征信息
* 标题和正文的换行并没有
* 在解析之前我是不是可以通过灰度，二值化之类的图片处理方式先将目标优化一下？
* 一些常用词语的识别错误是不是可以通过一些人工智能，或者猜词联想之类的工具，库来修复
  * e.g. 第一章第二段 '计算机这桦-个复杂的机体(organism)' 应该翻译为 '计算机这样一个复杂'
* 查一下中文和英文一起时，缩紧的格式如何才好看
* 还有中文中标点的使用，总感觉混在一起不好看
* 如果功能成熟了，我们是不是还可以给编辑以更优的操作体验，比如智能识别病句并给出提示之类的功能
* 如果对文学有足够多的了解的话，我们还可以根据不同作家的不同文体，对文章做修改，听上去很有意思的感觉
* 现有的文字识别好像会把段落末尾的句号删掉（PS：可能是我写的换行程序处理的时候将它删掉了）

## 处理时间记录

### 案例1：版本0.01

使用 pymupdf 做了简单的中文提取，统计在这个基础上处理一段文字所需要的时间。段落从 1.3.1 到 1.3.2，字数：1956，处理时间：26 mins。

感觉我现在处理的案例其实更精确的描述应该是从图片中提取文字才对，目标是如何更精确的提取图片中的内容。

下一个版本的改进中，可以先从最简单的方面做起，现在我刚摘录完第一章的前半部分，后半部分在修改文档的时候，可以先摘录一下解析出错的部分，然后通过看看通过图片的优化是否能提高识别率。罗列我当前的一些问题：

- [] pdf 默认的文字提取和存储为图片再提取效果有差别吗
- [] 通过改变 dpi 是否可以提高识别率
- [] 通过图片提取信息时，做一些二值化，灰度处理时候可以提高识别率

1.5-1.6 章错误记录

* 在结李第1章之前 - 结束
* 最货的还是最廉价的 - 最贵
* 最傻的计算机 - 最慢
* 只是更傲一些而已 - 更慢
* 但绝不会健得更多 - 做得
* 我们用英语或其他语言
* 太奇妙了1 - !
* 竣然被成功解决了 - 竟然
* 而丽这一复杂的转换任务 - 而且
* 不可怡议 - 思议
* 详细阅述 - 阐述
* 一本人门性 - 入门
* 先学计算机设备昵 - 呢
* 其原图在于 - 原理
* 能够计算的扔器 - 机器
* 有些是楼扔机 - 模拟机
* 印机器产生的结果 - 即
* 槲拟量 - 模拟
* 潘动其中的一个 - 滑动
* 绵后从第二个 - 然后
* 附离 - 距离
* 共工作原理 - 其
* 数孙机 - 数字机
* 这毓是为什么 - 就是
* 主家计算世界 - 主宰
* 慧悉 - 熟悉
* 数守手表 - 数字
* 健字母排序 - 做
* 怎样傲乔法 - 做乘法